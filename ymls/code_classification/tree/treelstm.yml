# Name of Configuration and Experiment Mode.
name: "CodeClassificationConfig"
task: "CodeClassification"
model_type: "TreeLSTM"
task_type: "Classification"
classify_mode: "Multi-class"

# Vocabulary Setting
token_vocab_path: "./data/poj104/token_vocab_dict.pkl"
node_vocab_path: "./data/poj104/node_vocab_dict.pkl"

# Dataset Specification Configuration
dataset:
  name: "online-judge"
  path: "./data/poj104/"

# General Configuration
output_path: "./trained_model/code_classification/treelstm"
use_cuda: True
disable_tqdm: True
initial_test: False
save_output: True
use_scheduler: True
class_weight: True
mm_var: "weighted_accuracy" # Main Monitor Variable/Main Metric Variable
monitor_vars: ["weighted_accuracy"]
gpu_id: 0

# Generic Model Hyperparameters
max_code_token_len: 20
class_num: 104
optimizer_type: ADAM
lr: 0.01
batch_size: 128
max_epoch: 50
patience: 10
word_emb_dims: 128 # For TreeLSTM, the embedding dims must be 2x of hidden size
dropout: 0.2
self_loop: False
reverse_edge: False
use_edge_type: ast # ast, cfg, cdg, ddg, and cpg

# Node Embedding Layer
node_emb_layer:
  mode: "LSTMEmbedNode" # Average/LSTM/MLP
  dims: 128
  layers: 1
  use_nfeature: "textual" # structure or textual or distinct-feats

# Tree LSTM Parameters
treelstm:
  in_dim: 64
  root_predict: False
  graph_agg: mean # sum, mean, max




